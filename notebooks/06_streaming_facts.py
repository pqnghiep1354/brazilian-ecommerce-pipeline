# Databricks notebook source
# MAGIC %md
# MAGIC # üì° 06 - Streaming Facts Pipeline
# MAGIC
# MAGIC ## Brazilian E-Commerce Pipeline
# MAGIC
# MAGIC **Objective:** Implement streaming pipeline for real-time fact table updates
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### üéØ Streaming Features:
# MAGIC - **Auto Loader:** Ingestion d·ªØ li·ªáu tƒÉng tr∆∞·ªüng (incremental).
# MAGIC - **Structured Streaming:** Bi·∫øn ƒë·ªïi d·ªØ li·ªáu li√™n t·ª•c.
# MAGIC - **Watermarking:** X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫øn mu·ªôn (late data).
# MAGIC - **foreachBatch:** H·ªó tr·ª£ th·ª±c hi·ªán logic `MERGE` ph·ª©c t·∫°p v√† lookup dimension.
# MAGIC - **Checkpointing:** ƒê·∫£m b·∫£o kh·∫£ nƒÉng ph·ª•c h·ªìi l·ªói (fault tolerance).

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîß Configuration & Setup

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql.streaming import StreamingQuery
from delta.tables import DeltaTable
from datetime import datetime
import time
import uuid

spark = SparkSession.builder.getOrCreate()
print("‚úÖ Libraries imported")

# COMMAND ----------

# DBTITLE 1,Configuration
class StreamingConfig:
    """C·∫•u h√¨nh t·∫≠p trung cho Unity Catalog."""
    CATALOG = "brazilian_ecommerce"
    BRONZE_SCHEMA = "bronze"
    SILVER_SCHEMA = "silver"
    BUSINESS_SCHEMA = "business"
    
    # Unity Catalog Volume Path
    VOLUME_PATH = f"/Volumes/{CATALOG}/{BRONZE_SCHEMA}/source_data"
    RAW_PATH = f"{VOLUME_PATH}/raw"
    CHECKPOINT_PATH = f"{VOLUME_PATH}/_checkpoints"
    SCHEMA_PATH = f"{VOLUME_PATH}/_schemas"
    
    # C·∫•u h√¨nh Trigger t·∫≠p trung
    # B·∫°n c√≥ th·ªÉ ƒë·ªïi sang "10 seconds", "1 minute" ho·∫∑c "availableNow" t·∫°i ƒë√¢y
    DEFAULT_TRIGGER = {"availableNow": True} 
    # Ho·∫∑c n·∫øu mu·ªën ch·∫°y theo chu k·ª≥:
    # DEFAULT_TRIGGER = {"processingTime": "1 minute"}
    
    # ƒê·ªãnh danh b·∫£ng 3 c·∫•p
    TABLE_BRONZE = f"{CATALOG}.{BRONZE_SCHEMA}.orders_streaming"
    TABLE_SILVER = f"{CATALOG}.{SILVER_SCHEMA}.orders_streaming"
    TABLE_FACT = f"{CATALOG}.{BUSINESS_SCHEMA}.fact_orders_streaming"
    TABLE_AGG = f"{CATALOG}.{BUSINESS_SCHEMA}.daily_orders_realtime"

config = StreamingConfig()

# COMMAND ----------

# MAGIC %md
# MAGIC ## üõ†Ô∏è Streaming Utilities

# COMMAND ----------

# DBTITLE 1,Streaming Utility Functions
class StreamingManager:
    """Qu·∫£n l√Ω c√°c lu·ªìng streaming v√† checkpoint."""
    def __init__(self):
        self.active_queries = {}
    
    def get_checkpoint(self, name):
        return f"{config.CHECKPOINT_PATH}/{name}"
    
    def get_schema_location(self, name):
        return f"{config.SCHEMA_PATH}/{name}"
    
    def stop_all(self):
        for name, query in self.active_queries.items():
            if query.isActive:
                query.stop()
                print(f"‚èπÔ∏è Stopped: {name}")
        self.active_queries = {}

stream_manager = StreamingManager()

def get_trigger(mode):
    return {"availableNow": True} if mode == "availableNow" else {"processingTime": mode}

# COMMAND ----------

# MAGIC %md
# MAGIC ## üì• Streaming Bronze Ingestion

# COMMAND ----------

# DBTITLE 1,Stream Orders to Bronze
def stream_orders_bronze(trigger_mode="availableNow"):
    """S·ª≠ d·ª•ng Auto Loader ƒë·ªÉ ƒë·∫©y d·ªØ li·ªáu v√†o b·∫£ng Bronze."""
    print(f"üöÄ Kh·ªüi ch·∫°y Bronze Stream...")
    
    df_stream = (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "csv")
        .option("cloudFiles.schemaLocation", f"{config.SCHEMA_PATH}/bronze_orders")
        .option("header", "true")
        .load(config.RAW_PATH))
    
    query = (df_stream
        .withColumn("_ingestion_timestamp", current_timestamp())
        .withColumn("_batch_id", lit(str(uuid.uuid4())))
        .withColumn("_source_file", col("_metadata.file_path")) # Thay ƒë·ªïi quan tr·ªçng t·∫°i ƒë√¢y
        .writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", stream_manager.get_checkpoint("bronze_orders"))
        .trigger(**get_trigger(trigger_mode))
        .toTable(config.TABLE_BRONZE))
    
    stream_manager.active_queries["bronze"] = query
    return query

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîÑ Streaming Silver Transformation

# COMMAND ----------

# DBTITLE 1,Stream Bronze to Silver
def stream_orders_silver(trigger_mode="availableNow"):
    """L√†m s·∫°ch v√† ƒë·ªãnh d·∫°ng l·∫°i d·ªØ li·ªáu ƒë∆°n h√†ng."""
    print(f"üöÄ Kh·ªüi ch·∫°y Silver Stream...")
    
    df_bronze = spark.readStream.table(config.TABLE_BRONZE)
    
    df_silver = (df_bronze
        .select(
            col("order_id").cast("string"),
            col("customer_id").cast("string"),
            lower(trim(col("order_status"))).alias("order_status"),
            to_timestamp("order_purchase_timestamp").alias("order_purchase_timestamp"),
            to_timestamp("order_delivered_customer_date").alias("order_delivered_customer_date"),
            to_timestamp("order_estimated_delivery_date").alias("order_estimated_delivery_date")
        )
        .withColumn("order_date", to_date("order_purchase_timestamp"))
        .withColumn("delivery_days", datediff("order_delivered_customer_date", "order_purchase_timestamp"))
        .withColumn("is_late_delivery", col("order_delivered_customer_date") > col("order_estimated_delivery_date"))
        .withColumn("_silver_at", current_timestamp()))
    
    query = (df_silver.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", stream_manager.get_checkpoint("silver_orders"))
        .trigger(**get_trigger(trigger_mode))
        .queryName("silver_transform")
        .toTable(config.TABLE_SILVER))
    
    stream_manager.active_queries["silver"] = query
    return query

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Streaming Fact Tables

# COMMAND ----------

# DBTITLE 1,Stream to Fact Orders
def stream_fact_orders(trigger_mode="availableNow"):
    """S·ª≠ d·ª•ng foreachBatch ƒë·ªÉ th·ª±c hi·ªán MERGE v√†o b·∫£ng Fact."""
    print(f"üöÄ Kh·ªüi ch·∫°y Fact Stream...")
    
    # Load dimensions (D√πng ƒë·ªÉ join n·∫øu c·∫ßn trong transform, hi·ªán t·∫°i ƒë·ªÉ ƒë√¢y ƒë·ªÉ b·∫°n tham kh·∫£o)
    # dim_cust = spark.table(f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.dim_customers").filter("_is_current = true")

    def merge_fact(batch_df, batch_id):
        # L∆ØU √ù: Ph·∫£i th·ª•t l·ªÅ to√†n b·ªô kh·ªëi code b√™n trong h√†m n√†y
        table_name = f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.fact_orders_streaming"
        
        # 1. Ki·ªÉm tra b·∫£ng t·ªìn t·∫°i b·∫±ng Catalog (Tr√°nh l·ªói Path must be absolute)
        if spark.catalog.tableExists(table_name):
            print(f"--- [Batch {batch_id}] ƒêang MERGE v√†o {table_name} ---")
            
            target_table = DeltaTable.forName(spark, table_name)
            
            (target_table.alias("t")
                .merge(
                    batch_df.alias("s"),
                    "t.order_id = s.order_id" 
                )
                .whenMatchedUpdateAll()
                .whenNotMatchedInsertAll()
                .execute())
        else:
            print(f"--- [Batch {batch_id}] ƒêang kh·ªüi t·∫°o b·∫£ng m·ªõi {table_name} ---")
            # 2. Kh·ªüi t·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i
            (batch_df.write
                .format("delta")
                .mode("overwrite") # Overwrite ·ªü ƒë√¢y ch·ªâ c√≥ t√°c d·ª•ng t·∫°o c·∫•u tr√∫c b·∫£ng l·∫ßn ƒë·∫ßu
                .saveAsTable(table_name))

    # 3. ƒê·ªãnh nghƒ©a Stream t·ª´ b·∫£ng Silver
    query = (spark.readStream
        .table(config.TABLE_SILVER)
        .writeStream
        .foreachBatch(merge_fact)
        .option("checkpointLocation", stream_manager.get_checkpoint("fact_orders"))
        .trigger(**get_trigger(trigger_mode))
        .queryName("fact_merge")
        .start())
    
    stream_manager.active_queries["fact"] = query
    return query

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Real-time Aggregations

# COMMAND ----------

# DBTITLE 1,Streaming Daily Aggregations
def stream_daily_aggregations(trigger_mode="availableNow"):
    """T√≠nh to√°n c√°c ch·ªâ s·ªë theo ng√†y s·ª≠ d·ª•ng Watermarking."""
    print(f"üöÄ Kh·ªüi ch·∫°y Aggregation Stream...")
    
    df_stream = (spark.readStream.table(config.TABLE_SILVER)
        .withWatermark("order_purchase_timestamp", "1 day")) # X·ª≠ l√Ω d·ªØ li·ªáu tr·ªÖ 1 ng√†y
    
    df_agg = (df_stream
        .groupBy(window("order_purchase_timestamp", "1 day"), "order_date")
        .agg(
            count("order_id").alias("order_count"),
            avg("delivery_days").alias("avg_delivery_days"),
            sum(when(col("is_late_delivery") == True, 1).otherwise(0)).alias("late_deliveries")
        )
        .select("order_date", "window.start", "window.end", "order_count", "avg_delivery_days", "late_deliveries",
                current_timestamp().alias("_updated_at")))
    
    query = (df_agg.writeStream
        .format("delta")
        .outputMode("complete") # Ch·∫ø ƒë·ªô ghi ƒë√® to√†n b·ªô b·∫£ng aggregation m·ªói batch
        .option("checkpointLocation", stream_manager.get_checkpoint("daily_aggs"))
        .trigger(**get_trigger(trigger_mode))
        .queryName("realtime_agg")
        .toTable(config.TABLE_AGG))
    
    stream_manager.active_queries["agg"] = query
    return query

# COMMAND ----------

# MAGIC %md
# MAGIC ## üöÄ Execute Streaming Pipeline

# COMMAND ----------

# DBTITLE 1,Run Full Streaming Pipeline
def run_full_pipeline(mode="availableNow"):
    start_time = datetime.now()
    
    # Th·ª© t·ª± th·ª±c thi tu·∫ßn t·ª± ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh to√†n v·∫πn d·ªØ li·ªáu
    q1 = stream_orders_bronze(mode)
    if mode == "availableNow": q1.awaitTermination()
    
    q2 = stream_orders_silver(mode)
    if mode == "availableNow": q2.awaitTermination()
    
    q3 = stream_fact_orders(mode)
    q4 = stream_daily_aggregations(mode)
    
    if mode == "availableNow":
        q3.awaitTermination()
        q4.awaitTermination()
        
    print(f"‚úÖ To√†n b·ªô pipeline ho√†n t·∫•t! T·ªïng th·ªùi gian: {datetime.now() - start_time}")

# Ch·∫°y pipeline
run_full_pipeline("availableNow")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Monitor Streaming Queries

# COMMAND ----------

# DBTITLE 1,Monitor Active Streams
def monitor_streams_advanced():
    """Monitor chi ti·∫øt hi·ªáu nƒÉng v√† ƒë·ªô tr·ªÖ c·ªßa c√°c lu·ªìng Stream."""
    active_streams = spark.streams.active
    
    print(f"\n{'='*80}")
    print(f"üìä B√ÅO C√ÅO GI√ÅM S√ÅT STREAMING - {spark.catalog.currentCatalog()}")
    print(f"{'='*80}\n")

    if not active_streams:
        print(" ‚ö†Ô∏è Kh√¥ng c√≥ lu·ªìng n√†o ƒëang ch·∫°y.")
        return

    for stream in active_streams:
        status = stream.status
        last_progress = stream.lastProgress
        
        print(f"üì° Query: {stream.name if stream.name else 'Unnamed'}")
        print(f"   ‚îú‚îÄ Status: {status['message']}")
        print(f"   ‚îú‚îÄ ID: {stream.id}")
        
        if last_progress:
            # T√≠nh to√°n c√°c ch·ªâ s·ªë quan tr·ªçng
            input_rows = last_progress.get('numInputRows', 0)
            process_rate = round(last_progress.get('processedRowsPerSecond', 0), 2)
            
            # L·∫•y th√¥ng tin v·ªÅ th·ªùi gian x·ª≠ l√Ω (Latency)
            duration = last_progress.get('durationMs', {})
            total_duration = sum(duration.values()) if duration else 0
            
            print(f"   ‚îú‚îÄ Input: {input_rows} rows")
            print(f"   ‚îú‚îÄ Speed: {process_rate} rows/sec")
            print(f"   ‚îú‚îÄ Latency (Batch Duration): {total_duration} ms")
            print(f"   ‚îî‚îÄ Checkpoint: {last_progress.get('sources')[0].get('description')[:50]}...")
        
        # Ki·ªÉm tra n·∫øu stream b·ªã l·ªói d·ª´ng ƒë·ªôt ng·ªôt
        if stream.exception():
            print(f"   ‚ùå ERROR: {stream.exception().get('message')[:100]}...")
        
        print("-" * 40)

# G·ªçi h√†m gi√°m s√°t
monitor_streams_advanced()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Verify Streaming Tables

# COMMAND ----------

# DBTITLE 1,Verify Streaming Tables
print(f"\n{'='*60}")
print("‚úÖ STREAMING TABLES VERIFICATION")
print(f"{'='*60}\n")

streaming_tables = [
    (f"{config.BRONZE_SCHEMA}.orders_streaming", "Bronze Orders"),
    (f"{config.SILVER_SCHEMA}.orders_streaming", "Silver Orders"),
    (f"{config.BUSINESS_SCHEMA}.fact_orders_streaming", "Fact Orders"),
    (f"{config.BUSINESS_SCHEMA}.daily_orders_realtime", "Daily Aggregations"),
]

for table_name, description in streaming_tables:
    try:
        df = spark.table(table_name)
        count = df.count()
        print(f"  ‚úÖ {description:<25} {count:>10,} records")
    except Exception as e:
        print(f"  ‚ö†Ô∏è {description:<25} Not created (may be expected)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîÑ Continuous Streaming (Optional)

# COMMAND ----------

# DBTITLE 1,Start Continuous Streaming (Optional)
# Uncomment to start continuous streaming with 10-second intervals

# def start_continuous_streaming():
#     """Start continuous streaming for real-time updates."""
#     
#     print("üîÑ Starting continuous streaming...")
#     
#     # Start all streams with processing time trigger
#     stream_orders_bronze("10 seconds")
#     stream_orders_silver("10 seconds")
#     stream_fact_orders("10 seconds")
#     stream_daily_aggregations("30 seconds")
#     
#     print("‚úÖ Continuous streaming started!")
#     print("   Run stream_manager.stop_all_streams() to stop")
#     
# # start_continuous_streaming()

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Final Summary

# COMMAND ----------

# DBTITLE 1,Streaming Pipeline Complete
print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üì° STREAMING PIPELINE COMPLETE                             ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  ‚úÖ Status:        SUCCESS                                                    ‚ïë
‚ïë  üìä Streams:       4 streaming pipelines configured                           ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  üìã Streaming Tables Created:                                                 ‚ïë
‚ïë     ‚Ä¢ bronze.orders_streaming       - Raw order ingestion                     ‚ïë
‚ïë     ‚Ä¢ silver.orders_streaming       - Transformed orders                      ‚ïë
‚ïë     ‚Ä¢ business.fact_orders_streaming - Fact table with dims                   ‚ïë
‚ïë     ‚Ä¢ business.daily_orders_realtime - Real-time aggregations                 ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  üîß Features Implemented:                                                     ‚ïë
‚ïë     ‚Ä¢ Auto Loader for incremental ingestion                                   ‚ïë
‚ïë     ‚Ä¢ Structured Streaming transformations                                    ‚ïë
‚ïë     ‚Ä¢ Watermarking for late data handling                                     ‚ïë
‚ïë     ‚Ä¢ foreachBatch for dimension lookups                                      ‚ïë
‚ïë     ‚Ä¢ Delta Lake merge for upserts                                            ‚ïë
‚ïë     ‚Ä¢ Checkpointing for fault tolerance                                       ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  ‚ö° Trigger Modes Available:                                                  ‚ïë
‚ïë     ‚Ä¢ availableNow - Process all available data (batch-like)                  ‚ïë
‚ïë     ‚Ä¢ processingTime("10 seconds") - Continuous micro-batches                 ‚ïë
‚ïë     ‚Ä¢ once - Process once and stop                                            ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  ‚è≠Ô∏è  Next Step:     Run 08_ml_models.py                                       ‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")