# Databricks notebook source
# MAGIC %md
# MAGIC # ü•â 02 - Bronze Layer Ingestion
# MAGIC
# MAGIC ## Brazilian E-Commerce Pipeline
# MAGIC
# MAGIC **Objective:** Ingest raw CSV data into Bronze Delta tables using Auto Loader
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### üéØ Bronze Layer Characteristics:
# MAGIC - Raw data with minimal transformation
# MAGIC - Schema inference and evolution support
# MAGIC - Metadata columns (ingestion timestamp, source file, batch ID)
# MAGIC - Incremental loading with Auto Loader
# MAGIC - Full audit trail

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, input_file_name, lit, expr, col
from pyspark.sql.types import *
import uuid
from datetime import datetime

# Kh·ªüi t·∫°o Spark
spark = SparkSession.builder.getOrCreate()

# COMMAND ----------

# ƒê·ªäNH NGHƒ®A BI·∫æN ·ªû ƒê·∫¶U NOTEBOOK
CATALOG_NAME = "brazilian_ecommerce"
BRONZE_SCHEMA = "bronze"
VOLUME_PATH = f"/Volumes/{CATALOG_NAME}/{BRONZE_SCHEMA}/source_data"

# Cell n√†y ph·∫£i ƒë∆∞·ª£c ch·∫°y ƒë·∫ßu ti√™n
class BronzeConfig:
    RAW_PATH = f"{VOLUME_PATH}/raw"
    CHECKPOINT_PATH = f"{VOLUME_PATH}/_checkpoints/bronze"
    SCHEMA_PATH = f"{VOLUME_PATH}/_schemas/bronze"
    
    # Mapping t√™n file CSV sang t√™n b·∫£ng ƒë√≠ch
    SOURCE_TABLES = {
        "customers": "olist_customers_dataset.csv",
        "orders": "olist_orders_dataset.csv",
        "order_items": "olist_order_items_dataset.csv",
        "products": "olist_products_dataset.csv",
        "sellers": "olist_sellers_dataset.csv",
        "payments": "olist_order_payments_dataset.csv",
        "reviews": "olist_order_reviews_dataset.csv",
        "geolocation": "olist_geolocation_dataset.csv",
        "category_translation": "product_category_name_translation.csv"
    }

print(f"‚úÖ ƒêang c·∫•u h√¨nh n·∫°p d·ªØ li·ªáu cho Catalog: {CATALOG_NAME}")

# COMMAND ----------

# DBTITLE 1,Utility Functions
# 2. Utility Functions
def add_bronze_metadata(df):
    """Th√™m c√°c c·ªôt metadata chuy√™n nghi·ªáp cho l·ªõp Bronze."""
    return df \
        .withColumn("_bronze_ingestion_ts", current_timestamp()) \
        .withColumn("_bronze_batch_id", lit(str(uuid.uuid4()))) \
        .withColumn("_bronze_source_file", col("_metadata.file_path")) \
        .withColumn("_bronze_row_id", expr("uuid()"))

def log_to_audit(table_name, status, records=0, error=None):
    """Ghi log v√†o b·∫£ng audit ƒë√£ t·∫°o ·ªü Notebook 02."""
    try:
        audit_data = [(str(uuid.uuid4()), table_name, "INGESTION", status, records, datetime.now())]
        audit_df = spark.createDataFrame(audit_data, ["event_id", "table_name", "operation", "status", "records_processed", "timestamp"])
        audit_df.write.format("delta").mode("append").saveAsTable(f"{CATALOG_NAME}.bronze._pipeline_audit")
    except:
        print(f"  ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng th·ªÉ ghi log cho {table_name}")

# COMMAND ----------

# DBTITLE 1,Auto Loader Ingestion Engine
def ingest_to_bronze(table_name, file_name):
    """S·ª≠ d·ª•ng Auto Loader ƒë·ªÉ n·∫°p d·ªØ li·ªáu t·ª´ Volume v√†o Delta Table."""
    print(f"üöÄ Processing: {table_name}...")
    
    # ƒê∆Ø·ªúNG D·∫™N TH∆Ø M·ª§C G·ªêC (Directory)
    source_directory = BronzeConfig.RAW_PATH 
    
    target_table = f"{CATALOG_NAME}.{BRONZE_SCHEMA}.{table_name}"
    checkpoint_dir = f"{BronzeConfig.CHECKPOINT_PATH}/{table_name}"
    schema_dir = f"{BronzeConfig.SCHEMA_PATH}/{table_name}"
    
    try:
        # S·ª≠ d·ª•ng readStream v·ªõi cloudFiles (Auto Loader)
        df_stream = (spark.readStream
            .format("cloudFiles")
            .option("cloudFiles.format", "csv")
            .option("cloudFiles.schemaLocation", schema_dir)
            .option("cloudFiles.inferColumnTypes", "true")
            .option("header", "true")
            .option("multiLine", "true")
            .option("escape", '"')
            # S·ª¨ D·ª§NG pathGlobFilter ƒê·ªÇ CH·ªåN FILE C·ª§ TH·ªÇ TRONG TH∆Ø M·ª§C
            .option("pathGlobFilter", file_name) 
            .load(source_directory)) # Truy·ªÅn TH∆Ø M·ª§C v√†o ƒë√¢y
        
        # Th√™m metadata v√† ghi xu·ªëng Delta Table
        query = (add_bronze_metadata(df_stream).writeStream
            .format("delta")
            .outputMode("append")
            .option("checkpointLocation", checkpoint_dir)
            .option("mergeSchema", "true")
            .trigger(availableNow=True)
            .toTable(target_table))
        
        query.awaitTermination()
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng record (S·ª≠ d·ª•ng spark.table ƒë·ªÉ l·∫•y s·ªë li·ªáu m·ªõi nh·∫•t)
        count = spark.table(target_table).count()
        log_to_audit(table_name, "SUCCESS", count)
        print(f"  ‚úÖ SUCCESS: {count:,} records n·∫°p v√†o {target_table}")
        
    except Exception as e:
        print(f"  ‚ùå FAILED: {str(e)}")
        log_to_audit(table_name, "FAILED", error=str(e))

# COMMAND ----------

# DBTITLE 1,Th·ª±c thi n·∫°p to√†n b·ªô c√°c b·∫£ng
# 4. Th·ª±c thi n·∫°p to√†n b·ªô c√°c b·∫£ng
print("üèÅ B·∫Øt ƒë·∫ßu ti·∫øn tr√¨nh n·∫°p d·ªØ li·ªáu l·ªõp Bronze...")

for target_table, csv_file in BronzeConfig.SOURCE_TABLES.items():
    ingest_to_bronze(target_table, csv_file)

# COMMAND ----------

# 5. T·ªïng k·∫øt & Ki·ªÉm tra
print(f"\n{'='*60}")
print("üìä T·ªîNG K·∫æT L·ªöP BRONZE")
print(f"{'='*60}")

for table in BronzeConfig.SOURCE_TABLES.keys():
    full_name = f"{CATALOG_NAME}.{BRONZE_SCHEMA}.{table}"
    try:
        cnt = spark.table(full_name).count()
        print(f"  ‚úÖ {full_name:<40} | Records: {cnt:,}")
    except:
        print(f"  ‚ùå {full_name:<40} | L·ªói: Kh√¥ng t√¨m th·∫•y b·∫£ng")

print(f"\n‚è≠Ô∏è B∆∞·ªõc ti·∫øp theo: Ch·∫°y notebook 04_silver_transformation.py")