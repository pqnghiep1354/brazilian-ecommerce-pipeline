# Databricks notebook source
# MAGIC %md
# MAGIC # ü§ñ 07 - Machine Learning Models
# MAGIC
# MAGIC ## Brazilian E-Commerce Pipeline
# MAGIC
# MAGIC **Objective:** Build ML models for customer segmentation, revenue forecasting, and anomaly detection
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### üéØ ML Models:
# MAGIC 1. **Customer Segmentation** - RFM clustering with K-Means
# MAGIC 2. **Revenue Forecasting** - Time series prediction
# MAGIC 3. **Anomaly Detection** - Revenue anomaly detection
# MAGIC 4. **Product Recommendation** - Association rules

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîß Configuration & Setup

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# ML Libraries
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer
from pyspark.ml.clustering import KMeans
from pyspark.ml.regression import LinearRegression, GBTRegressor
from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# MLflow
import mlflow
import mlflow.spark
from mlflow.tracking import MlflowClient

# Python ML
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler as SklearnScaler
from sklearn.cluster import KMeans as SklearnKMeans
from mlflow.models.signature import infer_signature
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error

spark = SparkSession.builder.getOrCreate()
print("‚úÖ Libraries imported")

# COMMAND ----------

# DBTITLE 1,Configuration
class MLConfig:
    """Configuration for ML models."""
    
    CATALOG = "brazilian_ecommerce"
    BRONZE_SCHEMA = "bronze"
    SILVER_SCHEMA = "silver"
    BUSINESS_SCHEMA = "business"
    SCHEMA_ML = "ml_models"

    # Unity Catalog Volume Path
    VOLUME_PATH = f"/Volumes/{CATALOG}/{BRONZE_SCHEMA}/source_data"
    RAW_PATH = f"{VOLUME_PATH}/raw"
    CHECKPOINT_PATH = f"{VOLUME_PATH}/_checkpoints"
    SCHEMA_PATH = f"{VOLUME_PATH}/_schemas"
    
    
    
    # L·∫•y d·ªØ li·ªáu t·ª´ b·∫£ng Fact orders m√† b·∫°n ƒë√£ x√¢y d·ª±ng ·ªü c√°c b∆∞·ªõc tr∆∞·ªõc
    TABLE_FACT_ORDERS = f"{CATALOG}.{BUSINESS_SCHEMA}.fact_orders_streaming"
    TABLE_SILVER_ITEMS = f"{CATALOG}.{SILVER_SCHEMA}.order_items"

    # MLflow settings
    CURRENT_USER = spark.sql("SELECT current_user()").collect()[0][0]
    EXPERIMENT_PATH = f"/Users/{CURRENT_USER}/brazilian_ecommerce_ml_v1"
    
    # Model parameters
    CUSTOMER_CLUSTERS = 5
    FORECAST_HORIZON  = 30  # D·ª± b√°o doanh thu 30 ng√†y t·ªõi
    ANOMALY_CONTAMINATION = 0.05 # T·ª∑ l·ªá ngo·∫°i l·ªá d·ª± ki·∫øn (5%)

config = MLConfig()

# --- H√ÄM B·ªî TR·ª¢ (HELPERS) ---
def get_table_path(table_name, schema=config.SCHEMA_ML):
    """Tr·∫£ v·ªÅ full path: catalog.schema.table"""
    return f"{config.CATALOG}.{schema}.{table_name}"

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîß Setup MLflow

# COMMAND ----------

# DBTITLE 1,Setup MLflow Experiment
# Thi·∫øt l·∫≠p MLflow s·ª≠ d·ª•ng Unity Catalog l√†m Model Registry
mlflow.set_registry_uri("databricks-uc") 

# Thi·∫øt l·∫≠p Tracking URI (b·∫Øt bu·ªôc cho Databricks Connect)
mlflow.set_tracking_uri("databricks")

try:
    # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n experiment l√† tuy·ªát ƒë·ªëi
    mlflow.set_experiment(config.EXPERIMENT_PATH)
    print(f"‚úÖ MLflow ƒë√£ s·∫µn s√†ng v·ªõi Unity Catalog.")
    print(f"üß™ Experiment Path: {config.EXPERIMENT_PATH}")
except Exception as e:
    # N·∫øu l·ªói do kh√¥ng c√≥ quy·ªÅn t·∫°o folder, th·ª≠ t·∫°o experiment th·ªß c√¥ng
    print(f"‚ö†Ô∏è ƒêang kh·ªüi t·∫°o Experiment m·ªõi...")
    mlflow.create_experiment(config.EXPERIMENT_PATH)
    mlflow.set_experiment(config.EXPERIMENT_PATH)

# COMMAND ----------

# MAGIC %md
# MAGIC ## üë• Model 1: Customer Segmentation (RFM Clustering)

# COMMAND ----------

# DBTITLE 1,Prepare Customer RFM Data
def prepare_rfm_data():
    print(f"üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu RFM...")
    
    # 1. ƒê·ªçc b·∫£ng Fact (Th√¥ng tin ƒë∆°n h√†ng & Kh√°ch h√†ng)
    # Ngu·ªìn: brazilian_ecommerce.business.fact_orders_streaming
    fact_df = spark.table(config.TABLE_FACT_ORDERS)
    
    # 2. ƒê·ªçc b·∫£ng Payments t·ª´ l·ªõp Silver
    # S·ª≠ d·ª•ng config.SILVER_SCHEMA
    payments_table = f"{config.CATALOG}.{config.SILVER_SCHEMA}.payments"
    print(f"üîó K·∫øt h·ª£p v·ªõi d·ªØ li·ªáu thanh to√°n t·ª´: {payments_table}")
    
    payments_df = spark.table(payments_table)
    
    # 3. T·ªïng h·ª£p gi√° tr·ªã thanh to√°n theo t·ª´ng order_id
    # Tr√°nh t√¨nh tr·∫°ng duplicate d√≤ng khi 1 ƒë∆°n h√†ng c√≥ nhi·ªÅu ph∆∞∆°ng th·ª©c thanh to√°n
    order_values = payments_df.groupBy("order_id").agg(
        sum("payment_value").alias("total_order_value")
    )
    
    # 4. Join Fact v·ªõi Payments
    joined_df = fact_df.join(order_values, "order_id", "inner")
    
    # 5. X√°c ƒë·ªãnh m·ªëc th·ªùi gian g·∫ßn nh·∫•t ƒë·ªÉ t√≠nh Recency
    max_date = joined_df.agg(max("order_purchase_timestamp")).collect()[0][0]
    
    # 6. T√≠nh to√°n 3 ch·ªâ s·ªë RFM
    rfm_df = joined_df.groupBy("customer_id").agg(
        datediff(lit(max_date), max("order_purchase_timestamp")).alias("recency"),
        count("order_id").alias("frequency"),
        sum("total_order_value").alias("monetary")
    ).filter("monetary > 0").dropna()
    
    print(f"‚úÖ Chu·∫©n b·ªã xong {rfm_df.count():,} kh√°ch h√†ng.")
    return rfm_df

# Th·ª±c thi chu·∫©n b·ªã d·ªØ li·ªáu
rfm_raw_data = prepare_rfm_data()

# COMMAND ----------

# DBTITLE 1,Train Customer Segmentation Model
def run_segmentation_pipeline_sklearn(rfm_spark_df):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n c·ª•m kh√°ch h√†ng b·∫±ng Scikit-Learn,
    t√≠ch h·ª£p MLflow Signature v√† l∆∞u k·∫øt qu·∫£ v√†o Unity Catalog.
    """
    print(f"\n{'='*60}")
    print(f"ü§ñ ƒêANG HU·∫§N LUY·ªÜN PH√ÇN C·ª§M (K={config.CUSTOMER_CLUSTERS})")
    print(f"{'='*60}")
    
    # 1. Chuy·ªÉn ƒë·ªïi sang Pandas ƒë·ªÉ x·ª≠ l√Ω tr√™n Driver (Tr√°nh l·ªói Whitelist c·ªßa Spark ML)
    print("‚è≥ ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang Pandas...")
    rfm_pdf = rfm_spark_df.toPandas()
    
    # X√°c ƒë·ªãnh c√°c c·ªôt ƒë·∫∑c tr∆∞ng (Features)
    feature_cols = ["recency", "frequency", "monetary"]
    X = rfm_pdf[feature_cols]
    
    # 2. Kh·ªüi ch·∫°y MLflow Tracking
    with mlflow.start_run(run_name="customer_segmentation_rfm_sklearn") as run:
        print("‚è≥ ƒêang chu·∫©n h√≥a d·ªØ li·ªáu v√† hu·∫•n luy·ªán m√¥ h√¨nh...")
        
        # 3. Chu·∫©n h√≥a d·ªØ li·ªáu (Scaling)
        scaler = SklearnScaler()
        X_scaled = scaler.fit_transform(X)
        
        # 4. Hu·∫•n luy·ªán m√¥ h√¨nh KMeans
        kmeans = SklearnKMeans(
            n_clusters=config.CUSTOMER_CLUSTERS, 
            random_state=42, 
            n_init=10
        )
        rfm_pdf['segment'] = kmeans.fit_predict(X_scaled)
        
        # 5. T·∫†O MLFLOW SIGNATURE (Chuy√™n nghi·ªáp h√≥a MLOps)
        # Signature gi√∫p ƒë·ªãnh nghƒ©a r√µ Input (recency, freq, mon) v√† Output (segment)
        signature = infer_signature(X, rfm_pdf[['segment']])
        
        # 6. Log tham s·ªë v√† m√¥ h√¨nh v√†o MLflow
        mlflow.log_param("n_clusters", config.CUSTOMER_CLUSTERS)
        mlflow.log_param("algorithm", "KMeans-Sklearn")
        
        mlflow.sklearn.log_model(
            sk_model=kmeans,
            artifact_path="customer_segmentation_model",
            signature=signature  # G·∫Øn signature v√†o model
        )
        
        # 7. G√°n nh√£n ph√¢n kh√∫c kh√°ch h√†ng
        print("‚è≥ ƒêang g√°n nh√£n ph√¢n kh√∫c v√† chu·∫©n b·ªã l∆∞u d·ªØ li·ªáu...")
        segment_map = {
            0: "Champions",
            1: "Loyal Customers",
            2: "Potential Loyalists",
            3: "At Risk",
            4: "Lost Customers"
        }
        rfm_pdf["segment_name"] = rfm_pdf["segment"].map(segment_map).fillna("Others")
        
        # 8. Chuy·ªÉn ng∆∞·ª£c l·∫°i Spark DataFrame v√† l∆∞u v√†o Unity Catalog
        final_spark_df = spark.createDataFrame(rfm_pdf) \
                              .withColumn("_updated_at", current_timestamp())
        
        output_table = f"{config.CATALOG}.{config.SCHEMA_ML}.customer_segments"
        
        final_spark_df.write \
            .format("delta") \
            .mode("overwrite") \
            .saveAsTable(output_table)
            
        print(f"‚úÖ TH√ÄNH C√îNG!")
        print(f"üìä MLflow Run ID: {run.info.run_id}")
        print(f"üíæ K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: {output_table}")
        
        return final_spark_df

# --- TH·ª∞C THI ---
if rfm_raw_data is not None:
    customer_segments = run_segmentation_pipeline_sklearn(rfm_raw_data)
    
    # Hi·ªÉn th·ªã th·ªëng k√™ nhanh
    display(customer_segments.groupBy("segment_name").count().orderBy(desc("count")))

# COMMAND ----------

# DBTITLE 1,Save Customer Segments
def save_customer_segments(predictions):
    """L∆∞u ph√¢n kh√∫c kh√°ch h√†ng v√†o Unity Catalog."""
    
    print(f"\n{'='*60}")
    print("üíæ ƒêANG L∆ØU K·∫æT QU·∫¢ PH√ÇN KH√öC KH√ÅCH H√ÄNG")
    print(f"{'='*60}")
    
    # 1. T·∫°o c·ªôt segment_name (N·∫øu trong h√†m train ch∆∞a l√†m)
    # N·∫øu h√†m run_segmentation_pipeline_sklearn ƒë√£ t·∫°o r·ªìi th√¨ b∆∞·ªõc n√†y c√≥ th·ªÉ b·ªè qua
    if "segment_name" not in predictions.columns:
        predictions = predictions.withColumn("segment_name",
            when(col("segment") == 0, "Champions")
            .when(col("segment") == 1, "Loyal Customers")
            .when(col("segment") == 2, "Potential Loyalists")
            .when(col("segment") == 3, "At Risk")
            .otherwise("Lost Customers"))
    
    # 2. Th√™m timestamp v√† ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt
    final_df = predictions.select(
    "customer_id", "recency", "frequency", 
    "monetary", "segment", "segment_name"
).withColumn("_updated_at", current_timestamp())
    
    # 3. ƒê·ªãnh nghƒ©a t√™n b·∫£ng theo Unity Catalog (catalog.schema.table)
    # S·ª≠ d·ª•ng h√†m helper c·ªßa b·∫°n ho·∫∑c vi·∫øt tr·ª±c ti·∫øp
    output_table = f"{config.CATALOG}.{config.SCHEMA_ML}.customer_segments"
    
    # 4. Ghi d·ªØ li·ªáu (Unity Catalog s·∫Ω t·ª± qu·∫£n l√Ω ƒë∆∞·ªùng d·∫´n v·∫≠t l√Ω)
    final_df.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(output_table)
    
    count = final_df.count()
    print(f"  ‚úÖ ƒê√£ l∆∞u {count:,} kh√°ch h√†ng v√†o b·∫£ng: {output_table}")
    
    return final_df

customer_segment_df = save_customer_segments(customer_segments)
display(customer_segment_df.orderBy(desc("customer_id")).limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìà Model 2: Revenue Forecasting

# COMMAND ----------

# DBTITLE 1,Prepare Time Series Data
def prepare_revenue_data():
    """Chu·∫©n b·ªã d·ªØ li·ªáu doanh thu theo ng√†y k√®m c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian."""
    print(f"üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu chu·ªói th·ªùi gian t·ª´ {config.TABLE_FACT_ORDERS}...")
    
    # X√°c ƒë·ªãnh b·∫£ng thanh to√°n trong Silver
    payments_table = f"{config.CATALOG}.{config.SILVER_SCHEMA}.payments"
    
    # K·∫øt h·ª£p Fact v√† Payments ƒë·ªÉ t√≠nh doanh thu h√†ng ng√†y
    ts_spark_df = spark.table(config.TABLE_FACT_ORDERS) \
        .join(spark.table(payments_table), "order_id") \
        .withColumn("ds", to_date("order_purchase_timestamp")) \
        .groupBy("ds").agg(
            sum("payment_value").alias("y"),         # Doanh thu th·ª±c t·∫ø
            count("order_id").alias("total_orders")  # S·ªë l∆∞·ª£ng ƒë∆°n h√†ng
        ) \
        .withColumn("day_of_week", dayofweek("ds")) \
        .withColumn("month", month("ds")) \
        .withColumn("day_of_month", dayofmonth("ds")) \
        .orderBy("ds").dropna()
    
    # Chuy·ªÉn sang Pandas ƒë·ªÉ x·ª≠ l√Ω Sklearn (Tr√°nh l·ªói Whitelist)
    pdf = ts_spark_df.toPandas()
    print(f"‚úÖ ƒê√£ chu·∫©n b·ªã {len(pdf)} ng√†y d·ªØ li·ªáu.")
    return pdf

# Th·ª±c thi chu·∫©n b·ªã d·ªØ li·ªáu
revenue_pdf = prepare_revenue_data()

# COMMAND ----------

# DBTITLE 1,Train Revenue Forecast Model
def train_revenue_model(pdf):
    """Hu·∫•n luy·ªán m√¥ h√¨nh GBR v√† log v√†o MLflow."""
    print(f"ü§ñ ƒêang hu·∫•n luy·ªán Gradient Boosting Regressor...")
    
    # ƒê·ªãnh nghƒ©a Features v√† Label
    features = ["day_of_week", "month", "day_of_month", "total_orders"]
    X = pdf[features]
    y = pdf["y"]

    with mlflow.start_run(run_name="revenue_forecast_gbr") as run:
        # Chia d·ªØ li·ªáu Train/Test (80% qu√° kh·ª©/20% hi·ªán t·∫°i)
        split_idx = int(len(pdf) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # Hu·∫•n luy·ªán
        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
        model.fit(X_train, y_train)
        
        # D·ª± b√°o v√† t·∫°o Signature
        test_preds = model.predict(X_test)
        signature = infer_signature(X_test, test_preds)
        
        # Log MLflow
        mlflow.log_param("forecast_horizon", config.FORECAST_HORIZON)
        mae = mean_absolute_error(y_test, test_preds)
        mlflow.log_metric("mae", mae)
        
        mlflow.sklearn.log_model(
            sk_model=model,
            artifact_path="revenue_model",
            signature=signature
        )
        
        print(f"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t. MAE: R$ {mae:,.2f}")
        return model, X, run.info.run_id

# Th·ª±c thi hu·∫•n luy·ªán
revenue_model, features_df, run_id = train_revenue_model(revenue_pdf)

# COMMAND ----------

# DBTITLE 1,Save Predictions to Unity Catalog
def save_forecast_results(model, pdf, features_df):
    """L∆∞u k·∫øt qu·∫£ d·ª± b√°o th·ª±c t·∫ø v√†o Unity Catalog."""
    print(f"üíæ ƒêang l∆∞u k·∫øt qu·∫£ d·ª± b√°o v√†o {config.SCHEMA_ML}...")
    
    # D·ª± b√°o cho to√†n b·ªô t·∫≠p d·ªØ li·ªáu
    pdf_results = pdf.copy()
    pdf_results['prediction'] = model.predict(features_df)
    
    # Chuy·ªÉn v·ªÅ Spark DataFrame v√† th√™m timestamp ƒë·ªìng b·ªô
    # L∆∞u √Ω: S·ª≠ d·ª•ng '_updated_at' ƒë·ªÉ kh·ªõp v·ªõi c√°c b·∫£ng tr∆∞·ªõc ƒë√≥
    forecast_spark_df = spark.createDataFrame(pdf_results) \
                             .withColumn("_updated_at", current_timestamp())
    
    output_table = f"{config.CATALOG}.{config.SCHEMA_ML}.revenue_forecasts"
    
    # Ghi ƒë√® b·∫£ng v√† c·∫•u h√¨nh t·ª± ƒë·ªông kh·ªõp schema
    forecast_spark_df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(output_table)
        
    print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ t·∫°i: {output_table}")
    return forecast_spark_df

# Th·ª±c thi l∆∞u tr·ªØ
forecast_final_df = save_forecast_results(revenue_model, revenue_pdf, features_df)

# Ki·ªÉm tra k·∫øt qu·∫£
display(forecast_final_df.orderBy(desc("ds")).limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç Model 3: Anomaly Detection

# COMMAND ----------

# DBTITLE 1,Prepare Data for Anomaly Detection
def prepare_anomaly_data():
    """Chu·∫©n b·ªã d·ªØ li·ªáu doanh thu v√† s·ªë l∆∞·ª£ng ƒë∆°n h√†ng h√†ng ng√†y ƒë·ªÉ ph√°t hi·ªán b·∫•t th∆∞·ªùng."""
    print(f"üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu t·ª´ {config.TABLE_FACT_ORDERS}...")
    
    payments_table = f"{config.CATALOG}.{config.SILVER_SCHEMA}.payments"
    
    # K·∫øt h·ª£p d·ªØ li·ªáu Fact v√† Payments
    ts_spark_df = spark.table(config.TABLE_FACT_ORDERS) \
        .join(spark.table(payments_table), "order_id") \
        .withColumn("ds", to_date("order_purchase_timestamp")) \
        .groupBy("ds").agg(
            sum("payment_value").alias("revenue"),
            count("order_id").alias("total_orders")
        ).orderBy("ds").dropna()
    
    # Chuy·ªÉn sang Pandas ƒë·ªÉ x·ª≠ l√Ω Sklearn
    pdf = ts_spark_df.toPandas()
    print(f"‚úÖ ƒê√£ chu·∫©n b·ªã {len(pdf)} ng√†y d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch b·∫•t th∆∞·ªùng.")
    return pdf

# Th·ª±c thi chu·∫©n b·ªã d·ªØ li·ªáu
anomaly_raw_pdf = prepare_anomaly_data()

# COMMAND ----------

# DBTITLE 1,Train Anomaly Detection Model
def train_anomaly_model(pdf):
    """Hu·∫•n luy·ªán m√¥ h√¨nh Isolation Forest v√† log v√†o MLflow."""
    print(f"ü§ñ ƒêang hu·∫•n luy·ªán Isolation Forest (Contamination={config.ANOMALY_CONTAMINATION})...")
    
    # ƒê·∫∑c tr∆∞ng d√πng ƒë·ªÉ ph√°t hi·ªán b·∫•t th∆∞·ªùng: Doanh thu v√† S·ªë l∆∞·ª£ng ƒë∆°n h√†ng
    features = ["revenue", "total_orders"]
    X = pdf[features]
    
    with mlflow.start_run(run_name="revenue_anomaly_detection") as run:
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaler = SklearnScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Hu·∫•n luy·ªán Isolation Forest
        model = IsolationForest(
            contamination=config.ANOMALY_CONTAMINATION,
            random_state=42
        )
        
        # D·ª± b√°o: -1 l√† b·∫•t th∆∞·ªùng, 1 l√† b√¨nh th∆∞·ªùng
        pdf['anomaly_score'] = model.fit_predict(X_scaled)
        pdf['is_anomaly'] = pdf['anomaly_score'].apply(lambda x: 1 if x == -1 else 0)
        
        # T·∫°o Signature cho MLflow
        signature = infer_signature(X, pdf[['is_anomaly']])
        
        # Log MLflow
        mlflow.log_param("contamination", config.ANOMALY_CONTAMINATION)
        mlflow.log_metric("total_anomalies", pdf['is_anomaly'].sum())
        
        mlflow.sklearn.log_model(
            sk_model=model,
            artifact_path="anomaly_model",
            signature=signature
        )
        
        print(f"‚úÖ Ph√°t hi·ªán {pdf['is_anomaly'].sum()} ƒëi·ªÉm b·∫•t th∆∞·ªùng.")
        return model, pdf, run.info.run_id

# Th·ª±c thi hu·∫•n luy·ªán
anomaly_model, anomaly_results_pdf, anomaly_run_id = train_anomaly_model(anomaly_raw_pdf)

# COMMAND ----------

# DBTITLE 1,Save Anomaly Detection Results
def save_anomaly_results(pdf):
    """L∆∞u danh s√°ch c√°c ƒëi·ªÉm b·∫•t th∆∞·ªùng v√†o Unity Catalog."""
    print(f"üíæ ƒêang l∆∞u k·∫øt qu·∫£ b·∫•t th∆∞·ªùng v√†o {config.SCHEMA_ML}...")
    
    # Chuy·ªÉn v·ªÅ Spark DataFrame v√† th√™m timestamp
    anomaly_spark_df = spark.createDataFrame(pdf) \
                            .withColumn("_updated_at", current_timestamp())
    
    output_table = f"{config.CATALOG}.{config.SCHEMA_ML}.revenue_anomalies"
    
    # Ghi ƒë√® b·∫£ng v√† c·∫•u h√¨nh t·ª± ƒë·ªông kh·ªõp schema
    anomaly_spark_df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(output_table)
        
    print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ t·∫°i: {output_table}")
    return anomaly_spark_df

# Th·ª±c thi l∆∞u tr·ªØ
anomaly_final_df = save_anomaly_results(anomaly_results_pdf)

# Hi·ªÉn th·ªã c√°c ng√†y c√≥ doanh thu b·∫•t th∆∞·ªùng
display(anomaly_final_df.filter("is_anomaly == 1").orderBy(desc("ds")))

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä Model 4: Product Affinity Analysis

# COMMAND ----------

# DBTITLE 1,Prepare Data for Product Affinity
def prepare_affinity_data():
    """Chu·∫©n b·ªã d·ªØ li·ªáu order_items t·ª´ l·ªõp Silver."""
    # L∆∞u √Ω: Ki·ªÉm tra t√™n b·∫£ng ch√≠nh x√°c c·ªßa b·∫°n trong silver (v√≠ d·ª•: 'order_items')
    table_name = config.TABLE_SILVER_ITEMS
    print(f"üìä ƒêang t·∫£i d·ªØ li·ªáu t·ª´ {table_name}...")
    
    order_items_df = spark.table(table_name)
    
    # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá d·ª±a tr√™n logic Pipeline c·ªßa b·∫°n
    if "_is_valid" in order_items_df.columns:
        order_items_df = order_items_df.filter(col("_is_valid") == True)
        
    print(f"‚úÖ ƒê√£ t·∫£i {order_items_df.count():,} b·∫£n ghi s·∫£n ph·∫©m.")
    return order_items_df

# Th·ª±c thi
affinity_input_df = prepare_affinity_data()

# COMMAND ----------

# DBTITLE 1,Analyze Product Affinity Logic
def analyze_affinity(order_items):
    """T√≠nh to√°n Support, Confidence v√† Lift cho c√°c c·∫∑p s·∫£n ph·∫©m."""
    print("‚è≥ ƒêang th·ª±c hi·ªán Self-join ƒë·ªÉ t√¨m c√°c c·∫∑p s·∫£n ph·∫©m mua c√πng nhau...")
    
    # 1. T√¨m c√°c c·∫∑p s·∫£n ph·∫©m (A, B) trong c√πng ƒë∆°n h√†ng
    # L·ªçc a.product_id < b.product_id ƒë·ªÉ kh√¥ng b·ªã l·∫∑p c·∫∑p (A,B) v√† (B,A)
    product_pairs = order_items.alias("a") \
        .join(order_items.alias("b"), "order_id") \
        .filter(col("a.product_id") < col("b.product_id")) \
        .select(
            col("a.product_id").alias("product_a"),
            col("b.product_id").alias("product_b")
        )
    
    # 2. ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói c·∫∑p
    pair_counts = product_pairs \
        .groupBy("product_a", "product_b") \
        .agg(count("*").alias("co_purchase_count")) \
        .filter("co_purchase_count >= 5") # Ch·ªâ l·∫•y c·∫∑p xu·∫•t hi·ªán t·ª´ 5 l·∫ßn tr·ªü l√™n
    
    # 3. T√≠nh to√°n c√°c ch·ªâ s·ªë Affinity
    total_orders = order_items.select("order_id").distinct().count()
    product_counts = order_items.groupBy("product_id").agg(countDistinct("order_id").alias("order_count"))
    
    affinity_df = pair_counts \
        .join(product_counts.alias("pa"), col("product_a") == col("pa.product_id")) \
        .withColumnRenamed("order_count", "product_a_count") \
        .join(product_counts.alias("pb"), col("product_b") == col("pb.product_id")) \
        .withColumnRenamed("order_count", "product_b_count") \
        .withColumn("support", col("co_purchase_count") / total_orders) \
        .withColumn("confidence_a_to_b", col("co_purchase_count") / col("product_a_count")) \
        .withColumn("lift", col("support") / ((col("product_a_count") / total_orders) * (col("product_b_count") / total_orders))) \
        .select("product_a", "product_b", "co_purchase_count", "support", "confidence_a_to_b", "lift")
        
    print(f"‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t. T√¨m th·∫•y {affinity_df.count():,} m·ªëi li√™n h·ªá s·∫£n ph·∫©m.")
    return affinity_df

# Th·ª±c thi ph√¢n t√≠ch
affinity_results_df = analyze_affinity(affinity_input_df)

# COMMAND ----------

# DBTITLE 1,Save Product Affinity to Unity Catalog
def save_affinity_results(affinity_df):
    """L∆∞u k·∫øt qu·∫£ v√†o b·∫£ng ml_models.product_affinity."""
    output_table = f"{config.CATALOG}.{config.SCHEMA_ML}.product_affinity"
    print(f"üíæ ƒêang l∆∞u k·∫øt qu·∫£ v√†o {output_table}...")
    
    # Th√™m timestamp v√† ghi ƒë√® schema ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng b·ªô
    final_df = affinity_df.withColumn("_updated_at", current_timestamp())
    
    final_df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(output_table)
        
    print(f"‚úÖ ƒê√£ l∆∞u b·∫£ng th√†nh c√¥ng!")
    return final_df

# Th·ª±c thi l∆∞u tr·ªØ
affinity_final = save_affinity_results(affinity_results_df)

# Hi·ªÉn th·ªã Top 10 c·∫∑p s·∫£n ph·∫©m c√≥ Lift cao nh·∫•t
display(affinity_final.orderBy(desc("lift")).limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Verify ML Tables

# COMMAND ----------

# DBTITLE 1,Verify ML Tables
# ‚úÖ ML Tables Verification
print(f"\n{'='*70}")
print(f"‚úÖ KI·ªÇM TRA H·ªÜ TH·ªêNG B·∫¢NG ML - CATALOG: {config.CATALOG}")
print(f"{'='*70}\n")

# Danh s√°ch c√°c b·∫£ng m·ª•c ti√™u ƒë√£ ƒë∆∞·ª£c t·∫°o t·ª´ 4 Model
ml_tables = [
    "customer_segments",
    "revenue_forecasts",
    "revenue_anomalies",
    "product_affinity"
]

for table in ml_tables:
    try:
        # X√¢y d·ª±ng full path theo c·∫•u tr√∫c: catalog.ml_models.table_name
        full_table_name = f"{config.CATALOG}.{config.SCHEMA_ML}.{table}"
        
        # ƒê·ªçc th·ª≠ b·∫£ng t·ª´ Unity Catalog
        df = spark.table(full_table_name)
        count = df.count()
        
        print(f"  ‚úÖ {table:<25} | S·ªë l∆∞·ª£ng b·∫£n ghi: {count:>10,}")
        
        # Ki·ªÉm tra timestamp c·∫≠p nh·∫≠t g·∫ßn nh·∫•t (n·∫øu c√≥ c·ªôt _updated_at)
        if "_updated_at" in df.columns:
            last_update = df.agg(max("_updated_at")).collect()[0][0]
            print(f"     ‚îî‚îÄ C·∫≠p nh·∫≠t g·∫ßn nh·∫•t: {last_update}")
            
    except Exception as e:
        # Hi·ªÉn th·ªã l·ªói ng·∫Øn g·ªçn n·∫øu b·∫£ng ch∆∞a t·ªìn t·∫°i ho·∫∑c b·ªã l·ªói schema
        error_msg = str(e).split('\n')[0][:50]
        print(f"  ‚ùå {table:<25} | L·ªñI: {error_msg}...")

print(f"\n{'='*70}")
print("üèÅ HO√ÄN T·∫§T PIPELINE MACHINE LEARNING")
print(f"{'='*70}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä MLflow Model Registry

# COMMAND ----------

# DBTITLE 1,List MLflow Runs
# üìä Ki·ªÉm tra l·ªãch s·ª≠ ch·∫°y Experiment & Model Registry
print(f"\n{'='*70}")
print(f"üìä MLFLOW TRACKING & REGISTRY REPORT")
print(f"{'='*70}\n")

try:
    client = MlflowClient()
    
    # 1. Truy xu·∫•t th√¥ng tin Experiment
    # S·ª≠ d·ª•ng config.EXPERIMENT_PATH ƒë√£ ƒë·ªãnh nghƒ©a ·ªü ph·∫ßn ƒë·∫ßu Notebook
    experiment = client.get_experiment_by_name(config.EXPERIMENT_PATH)
    
    if experiment:
        print(f"üß™ Experiment Name: {experiment.name}")
        print(f"üÜî Experiment ID: {experiment.experiment_id}")
        
        # T√¨m ki·∫øm 10 l·∫ßn ch·∫°y g·∫ßn nh·∫•t
        runs = client.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=["start_time DESC"],
            max_results=10
        )
        
        print(f"\n{'Run Name':<35} | {'Status':<12} | {'Duration':>8}")
        print("-" * 70)
        
        for run in runs:
            run_name = run.data.tags.get("mlflow.runName", "Unnamed Run")
            status = run.info.status
            # T√≠nh to√°n th·ªùi gian ch·∫°y (ms -> s)
            duration = (run.info.end_time - run.info.start_time) / 1000 if run.info.end_time else 0
            print(f"{run_name:<35} | {status:<12} | {duration:>7.1f}s")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Experiment t·∫°i ƒë∆∞·ªùng d·∫´n: {config.EXPERIMENT_PATH}")
    
except Exception as e:
    print(f"‚ö†Ô∏è L·ªói khi truy xu·∫•t d·ªØ li·ªáu MLflow: {e}")

print(f"\n{'='*70}")