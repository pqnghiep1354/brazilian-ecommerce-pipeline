# Databricks notebook source
# MAGIC %md
# MAGIC # âš™ï¸ 01 - Setup Databricks Environment
# MAGIC
# MAGIC ## Brazilian E-Commerce Pipeline
# MAGIC
# MAGIC **Objective:** Configure Databricks environment for the data pipeline
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### ğŸ¯ Setup Tasks:
# MAGIC 1. Create Unity Catalog (if available) or Hive Metastore schemas
# MAGIC 2. Setup Bronze, Silver, Gold, Business databases/schemas
# MAGIC 3. Configure Delta Lake settings
# MAGIC 4. Set up checkpoint directories
# MAGIC 5. Create utility functions
# MAGIC 6. Validate environment

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ Configuration

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import os
from datetime import datetime
import json

# 1. Khá»Ÿi táº¡o Spark (Bá» sparkContext Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch Serverless)
spark = SparkSession.builder.getOrCreate()

print("âœ… Spark Session initialized")
print(f"   Spark Version: {spark.version}")

# COMMAND ----------

# DBTITLE 1,Configuration Parameters
# 2. # Cáº¥u hÃ¬nh Ä‘á»‹nh danh dá»± Ã¡n
CATALOG_NAME = "brazilian_ecommerce"
SCHEMAS = ["bronze", "silver", "gold", "business", "ml_models"]

# Cáº¥u hÃ¬nh lÆ°u trá»¯ trong Unity Catalog Volume
VOLUME_PATH = f"/Volumes/{CATALOG_NAME}/bronze/source_data"

class Config:
    RAW_PATH = f"{VOLUME_PATH}/raw"
    CHECKPOINT_PATH = f"{VOLUME_PATH}/_checkpoints"
    SCHEMA_PATH = f"{VOLUME_PATH}/_schemas"
    
    @staticmethod
    def table(schema, table_name):
        return f"{CATALOG_NAME}.{schema}.{table_name}"

print(f"âœ… Äang khá»Ÿi táº¡o mÃ´i trÆ°á»ng cho Catalog: {CATALOG_NAME}")

# COMMAND ----------

# 3. Khá»Ÿi táº¡o Cáº¥u trÃºc Unity Catalog
# Táº¡o Catalog
spark.sql(f"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}")

# Táº¡o cÃ¡c Schemas (Bronze, Silver, Gold, Business, ML)
for schema in SCHEMAS:
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{schema}")
    print(f"  âœ… ÄÃ£ táº¡o Schema: {CATALOG_NAME}.{schema}")

# Táº¡o Volume Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u váº­t lÃ½ (File CSV, Checkpoints)
spark.sql(f"CREATE VOLUME IF NOT EXISTS {CATALOG_NAME}.bronze.source_data")
print(f"  âœ… ÄÃ£ táº¡o Volume táº¡i: {VOLUME_PATH}")

# COMMAND ----------

# 4. Khá»Ÿi táº¡o Cáº¥u trÃºc ThÆ° má»¥c (Volumes)
print("ğŸ“ Äang cáº¥u trÃºc thÆ° má»¥c trong Volume...")

paths_to_create = [
    Config.RAW_PATH, 
    Config.CHECKPOINT_PATH, 
    Config.SCHEMA_PATH,
    f"{Config.CHECKPOINT_PATH}/bronze",
    f"{Config.CHECKPOINT_PATH}/silver"
]

for path in paths_to_create:
    try:
        dbutils.fs.mkdirs(path)
        print(f"  âœ… ÄÃ£ táº¡o thÆ° má»¥c: {path}")
    except Exception as e:
        print(f"  âš ï¸ Cáº£nh bÃ¡o khi táº¡o {path}: {str(e)}")

# COMMAND ----------

# 5. Cáº¥u hÃ¬nh Spark (Fail-safe cho Serverless)
# CÃ¡c tham sá»‘ cáº¥u hÃ¬nh Spark (Sá»­ dá»¥ng try-except Ä‘á»ƒ bá» qua náº¿u bá»‹ Serverless khÃ³a)
spark_conf = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.shuffle.partitions": "auto",
    "spark.databricks.delta.schema.autoMerge.enabled": "true",
    "spark.sql.streaming.schemaInference": "true"
}

print("âš¡ Äang Ã¡p dá»¥ng cáº¥u hÃ¬nh Spark...")
for key, value in spark_conf.items():
    try:
        spark.conf.set(key, value)
        print(f"  âœ… {key} = {value}")
    except Exception as e:
        print(f"  âš ï¸ Bá» qua {key}: Há»‡ thá»‘ng Serverless tá»± quáº£n lÃ½ (Lá»—i 42K0I)")

# COMMAND ----------

# 6. Khá»Ÿi táº¡o Há»‡ thá»‘ng Logging (Audit Table)
print("ğŸ“Š Äang khá»Ÿi táº¡o báº£ng quáº£n lÃ½ Pipeline...")

audit_schema = StructType([
    StructField("event_id", StringType(), False),
    StructField("table_name", StringType(), False),
    StructField("operation", StringType(), False),
    StructField("status", StringType(), False),
    StructField("records_processed", LongType(), True),
    StructField("timestamp", TimestampType(), False)
])

# Táº¡o báº£ng Delta rá»—ng trong schema Bronze Ä‘á»ƒ lÆ°u log cháº¡y pipeline
spark.createDataFrame([], audit_schema).write \
    .format("delta") \
    .mode("ignore") \
    .saveAsTable(Config.table("bronze", "_pipeline_audit"))

print(f"  âœ… Báº£ng Audit sáºµn sÃ ng: {CATALOG_NAME}.bronze._pipeline_audit")

# COMMAND ----------

# 7. XÃ¡c nháº­n Tráº¡ng thÃ¡i Cuá»‘i cÃ¹ng
print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ¨ SETUP HOÃ€N Táº¤T - Há»† THá»NG Sáº´N SÃ€NG!                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Catalog:    {CATALOG_NAME:<44} â•‘
â•‘ Volume Root: {VOLUME_PATH:<44} â•‘
â•‘ Raw Data:    {Config.RAW_PATH:<44} â•‘
â•‘ Status:     SUCCESS                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â­ï¸ BÆ°á»›c tiáº¿p theo: Cháº¡y notebook 01_download_data.py Ä‘á»ƒ náº¡p dá»¯ liá»‡u.
""")

# COMMAND ----------

# %sql
# -- Lá»‡nh CASCADE cá»±c ká»³ quan trá»ng vÃ¬ nÃ³ sáº½ xÃ³a toÃ n bá»™ 
# -- Schema (bronze, silver, gold...) vÃ  dá»¯ liá»‡u bÃªn trong.
# DROP CATALOG IF EXISTS brazilian_ecommerce CASCADE;

# COMMAND ----------

# # Kiá»ƒm tra danh sÃ¡ch file trong Volume
# files = dbutils.fs.ls("/Volumes/brazilian_ecommerce/bronze/source_data/raw")
# for f in files:
#     print(f"âœ… ÄÃ£ tÃ¬m tháº¥y: {f.name} ({f.size / 1024 / 1024:.2f} MB)")