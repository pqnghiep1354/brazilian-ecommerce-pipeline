# Databricks notebook source
# MAGIC %md
# MAGIC # ü•à 03 - Silver Layer Transformation
# MAGIC
# MAGIC ## Brazilian E-Commerce Pipeline
# MAGIC
# MAGIC **Objective:** Transform Bronze data into cleansed, validated Silver tables
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### üéØ Silver Layer Characteristics:
# MAGIC - Data type casting and standardization
# MAGIC - Null handling and default values
# MAGIC - Deduplication and data cleansing
# MAGIC - Business rules validation
# MAGIC - Referential integrity checks
# MAGIC - Data quality metrics

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîß Configuration & Setup

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from datetime import datetime
import uuid

spark = SparkSession.builder.getOrCreate()

print("‚úÖ Libraries imported successfully")

# COMMAND ----------

# DBTITLE 1,Configuration
# C·∫•u h√¨nh ƒë·ªãnh danh (ƒê·ªìng b·ªô v·ªõi Notebook 02 & 03)
CATALOG_NAME = "brazilian_ecommerce"
BRONZE_SCHEMA = "bronze"
SILVER_SCHEMA = "silver"
VOLUME_PATH = f"/Volumes/{CATALOG_NAME}/{BRONZE_SCHEMA}/source_data"

class SilverConfig:
    # Danh s√°ch ƒë·∫ßy ƒë·ªß 9 b·∫£ng t·ª´ b∆∞·ªõc Bronze Ingestion
    TABLE_MAP = {
        "customers": ["customer_id"],
        "orders": ["order_id"],
        "order_items": ["order_id", "order_item_id"],
        "products": ["product_id"],
        "sellers": ["seller_id"],
        "payments": ["order_id", "payment_sequential"],
        "reviews": ["review_id"],
        "geolocation": ["geolocation_zip_code_prefix", "geolocation_lat", "geolocation_lng"],
        "category_translation": ["product_category_name"]
    }

print(f"‚úÖ ƒêang th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi cho Catalog: {CATALOG_NAME}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üõ†Ô∏è Utility Functions

# COMMAND ----------

# DBTITLE 1,Utility Functions
# 2. Utility Functions
def add_silver_metadata(df):
    """Th√™m c·ªôt metadata chu·∫©n cho l·ªõp Silver."""
    return df \
        .withColumn("_silver_timestamp", current_timestamp()) \
        .withColumn("_silver_batch_id", lit(str(uuid.uuid4()))) \
        .withColumn("_is_valid", lit(True))

def deduplicate_data(df, primary_keys):
    """
    Lo·∫°i b·ªè tr√πng l·∫∑p b·∫±ng Window Function.
    Gi·ªØ l·∫°i b·∫£n ghi c√≥ th·ªùi gian n·∫°p Bronze m·ªõi nh·∫•t.
    """
    if not primary_keys: return df
    
    # S·∫Øp x·∫øp theo th·ªùi gian n·∫°p ·ªü l·ªõp Bronze (ƒë√£ t·∫°o ·ªü Notebook 03)
    window = Window.partitionBy(primary_keys).orderBy(col("_bronze_ingestion_ts").desc())
    
    return df.withColumn("row_num", row_number().over(window)) \
             .filter(col("row_num") == 1) \
             .drop("row_num")

def log_audit(table_name, status, count=0):
    """Ghi log v√†o b·∫£ng audit t·∫≠p trung t·∫°i l·ªõp Bronze."""
    try:
        audit_data = [(str(uuid.uuid4()), table_name, "SILVER_TRANSFORM", status, count, datetime.now())]
        spark.createDataFrame(audit_data, ["event_id", "table_name", "operation", "status", "records_processed", "timestamp"]) \
             .write.format("delta").mode("append").saveAsTable(f"{CATALOG_NAME}.bronze._pipeline_audit")
    except:
        pass

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîÑ Transformation Functions

# COMMAND ----------

# DBTITLE 1,Main Transformation Engine
# 3. Main Transformation Engine
def transform_to_silver(table_name, pk_cols):
    print(f"üîÑ Chuy·ªÉn ƒë·ªïi b·∫£ng: {table_name}")
    
    source_table = f"{CATALOG_NAME}.{BRONZE_SCHEMA}.{table_name}"
    target_table = f"{CATALOG_NAME}.{SILVER_SCHEMA}.{table_name}"
    
    df = spark.table(source_table)
    
    # 1. L√†m s·∫°ch chung: Trim kho·∫£ng tr·∫Øng
    for c in df.columns:
        if isinstance(df.schema[c].dataType, StringType):
            df = df.withColumn(c, trim(col(c)))

    # 2. ƒê·ªãnh nghƒ©a danh s√°ch c·ªôt Timestamp c·ª• th·ªÉ cho t·ª´ng b·∫£ng ƒë·ªÉ tr√°nh √©p ki·ªÉu nh·∫ßm c·ªôt 'status'
    timestamp_mapping = {
        "orders": [
            "order_purchase_timestamp", "order_approved_at", 
            "order_delivered_carrier_date", "order_delivered_customer_date", 
            "order_estimated_delivery_date"
        ],
        "order_items": ["shipping_limit_date"],
        "reviews": ["review_creation_date", "review_answer_timestamp"]
    }

    # Th·ª±c hi·ªán √©p ki·ªÉu TIMESTAMP an to√†n cho c√°c c·ªôt ƒë√£ ƒë·ªãnh nghƒ©a
    if table_name in timestamp_mapping:
        for c in timestamp_mapping[table_name]:
            if c in df.columns:
                # to_timestamp m·∫∑c ƒë·ªãnh s·∫Ω tr·∫£ v·ªÅ NULL n·∫øu format kh√¥ng kh·ªõp, kh√¥ng g√¢y crash
                df = df.withColumn(c, to_timestamp(col(c)))

    # 3. Chu·∫©n h√≥a ri√™ng cho c√°c c·ªôt ƒë·ªãa l√Ω v√† ti·ªÅn t·ªá
    if table_name in ["customers", "sellers", "geolocation"]:
        zip_col = "geolocation_zip_code_prefix" if table_name == "geolocation" else f"{table_name[:-1]}_zip_code_prefix"
        df = df.withColumn(zip_col, lpad(col(zip_col).cast("string"), 5, "0"))
        
    if table_name in ["order_items", "payments", "products"]:
        decimal_cols = ["price", "freight_value", "payment_value", "product_weight_g", 
                        "product_length_cm", "product_height_cm", "product_width_cm"]
        for c in decimal_cols:
            if c in df.columns:
                df = df.withColumn(c, col(c).cast("decimal(10,2)"))

    # 4. Kh·ª≠ tr√πng d·ªØ li·ªáu d·ª±a tr√™n PK v√† th·ªùi gian n·∫°p Bronze m·ªõi nh·∫•t
    df = deduplicate_data(df, pk_cols)
    
    # 5. Th√™m metadata Silver
    df = add_silver_metadata(df)
    
    # 6. Ghi d·ªØ li·ªáu xu·ªëng l·ªõp Silver
    df.write.format("delta") \
      .mode("overwrite") \
      .option("overwriteSchema", "true") \
      .saveAsTable(target_table)
    
    final_count = df.count()
    log_audit(table_name, "SUCCESS", final_count)
    print(f"  ‚úÖ Ho√†n t·∫•t: {final_count:,} d√≤ng ƒë√£ ghi v√†o {target_table}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üöÄ Execute Silver Transformation

# COMMAND ----------

# DBTITLE 1,Run All Silver Transformations
print("üèÅ B·∫Øt ƒë·∫ßu ti·∫øn tr√¨nh Silver Transformation...")

for table, pks in SilverConfig.TABLE_MAP.items():
    try:
        transform_to_silver(table, pks)
    except Exception as e:
        print(f"  ‚ùå L·ªói t·∫°i b·∫£ng {table}: {str(e)}")
        log_audit(table, "FAILED")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Verify Silver Tables

# COMMAND ----------

# DBTITLE 1,Verify Silver Tables
# %sql
# -- 5. Verify Results
# -- Ki·ªÉm tra tr·∫°ng th√°i n·∫°p d·ªØ li·ªáu t·ª´ b·∫£ng Audit
# SELECT table_name, operation, status, records_processed, timestamp 
# FROM brazilian_ecommerce.bronze._pipeline_audit 
# WHERE operation = 'SILVER_TRANSFORM'
# ORDER BY timestamp DESC;

# COMMAND ----------

# DBTITLE 1,PySpark Automation Check
# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col

# def check_pipeline_status(table_name=None):
#     """
#     Ki·ªÉm tra tr·∫°ng th√°i n·∫°p d·ªØ li·ªáu t·ª´ b·∫£ng audit.
#     N·∫øu tr·∫°ng th√°i l√† 'FAIL', d·ª´ng pipeline b·∫±ng c√°ch raise Exception.
#     """
#     print("Checking pipeline audit status...")
    
#     # 1. Truy v·∫•n b·∫£n ghi m·ªõi nh·∫•t c·ªßa ti·∫øn tr√¨nh SILVER_TRANSFORM
#     query = """
#         SELECT table_name, operation, status, records_processed, timestamp 
#         FROM brazilian_ecommerce.bronze._pipeline_audit 
#         WHERE operation = 'SILVER_TRANSFORM'
#         ORDER BY timestamp DESC 
#         LIMIT 1
#     """
    
#     audit_df = spark.sql(query)
    
#     # 2. Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu audit hay kh√¥ng
#     if audit_df.count() == 0:
#         raise Exception("CRITICAL: Kh√¥ng t√¨m th·∫•y l·ªãch s·ª≠ v·∫≠n h√†nh trong b·∫£ng audit!")
    
#     # L·∫•y th√¥ng tin b·∫£n ghi m·ªõi nh·∫•t
#     latest_run = audit_df.first()
#     status = latest_run['status']
#     target_table = latest_run['table_name']
#     records = latest_run['records_processed']
    
#     print(f"Table: {target_table} | Status: {status} | Records: {records}")

#     # 3. Logic ki·ªÉm tra v√† d·ª´ng Pipeline
#     if status.upper() == 'FAIL':
#         error_msg = f"PIPELINE STOPPED: Ti·∫øn tr√¨nh n·∫°p b·∫£ng {target_table} th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra log!"
#         raise Exception(error_msg)
    
#     elif status.upper() == 'SUCCESS' and records == 0:
#         # Tr∆∞·ªùng h·ª£p th√†nh c√¥ng nh∆∞ng kh√¥ng c√≥ d·ªØ li·ªáu (c√≥ th·ªÉ l√† c·∫£nh b√°o)
#         print("WARNING: Ti·∫øn tr√¨nh th√†nh c√¥ng nh∆∞ng kh√¥ng c√≥ b·∫£n ghi n√†o ƒë∆∞·ª£c x·ª≠ l√Ω.")
    
#     else:
#         print(f"SUCCESS: Ti·∫øn tr√¨nh cho b·∫£ng {target_table} ƒë√£ ho√†n th√†nh t·ªët.")

# # Th·ª±c thi h√†m ki·ªÉm tra
# try:
#     check_pipeline_status()
# except Exception as e:
#     # N·∫øu ch·∫°y trong Databricks Workflow, raise Exception s·∫Ω l√†m Task n√†y b·ªã Mark l√† Failed
#     raise e

# COMMAND ----------

# DBTITLE 1,Data Quality - DQ
# def quality_gate_silver_orders():
#     # Ki·ªÉm tra kh√¥ng ƒë∆∞·ª£c c√≥ ID tr√πng l·∫∑p ·ªü t·∫ßng Silver
#     duplicate_count = spark.sql("SELECT order_id FROM brazilian_ecommerce.silver.orders GROUP BY order_id HAVING COUNT(*) > 1").count()
    
#     if duplicate_count > 0:
#         raise Exception(f"DATA QUALITY FAILED: Ph√°t hi·ªán {duplicate_count} d√≤ng tr√πng l·∫∑p trong b·∫£ng Silver Orders!")
    
#     print("Data Quality Check Passed: No duplicates found.")

# # G·ªçi h√†m sau khi ki·ªÉm tra Audit Status
# quality_gate_silver_orders()

# COMMAND ----------

