# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸš¨ 08 - Revenue Alerts System
# MAGIC
# MAGIC ## Brazilian E-Commerce Pipeline
# MAGIC
# MAGIC **Objective:** Implement automated alerts for revenue anomalies and business metrics
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ### ğŸ¯ Alert Types:
# MAGIC 1. **Revenue Spike Alert** - Sudden revenue increase (>50%)
# MAGIC 2. **Revenue Drop Alert** - Sudden revenue decrease (>30%)
# MAGIC 3. **Anomaly Alert** - ML-detected anomalies
# MAGIC 4. **KPI Threshold Alert** - Business metrics thresholds
# MAGIC 5. **Data Quality Alert** - Data freshness and completeness

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ Configuration & Setup

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from datetime import datetime, timedelta
import json
import uuid
import requests

spark = SparkSession.builder.getOrCreate()
print("âœ… Libraries imported")

# COMMAND ----------

# DBTITLE 1,Configuration
class AlertConfig:
    CATALOG = "brazilian_ecommerce"
    SILVER_SCHEMA = "silver"
    BUSINESS_SCHEMA = "business"
    ML_SCHEMA = "ml_models"
    
    # NgÆ°á»¡ng cáº£nh bÃ¡o (Thresholds)
    SPIKE_THRESHOLD = 0.50  # TÄƒng 50%
    DROP_THRESHOLD = -0.30  # Giáº£m 30%
    Z_SCORE_THRESHOLD = 3.0
    LOOKBACK_DAYS = 7
    
    # Cáº¥u hÃ¬nh thÃ´ng bÃ¡o (Placeholder)
    SLACK_WEBHOOK = None  
    EMAIL_ENABLED = False

config = AlertConfig()

print(f"âœ… Alert System synchronized with Catalog: {config.CATALOG}")

# COMMAND ----------

# Äá»‹nh nghÄ©a Schema tÆ°á»ng minh Ä‘á»ƒ trÃ¡nh lá»—i [CANNOT_DETERMINE_TYPE]
ALERT_SCHEMA = StructType([
    StructField("alert_id", StringType(), False),
    StructField("alert_type", StringType(), False),
    StructField("severity", StringType(), False),
    StructField("title", StringType(), False),
    StructField("message", StringType(), False),
    StructField("metric_name", StringType(), True),
    StructField("metric_value", DoubleType(), True),
    StructField("threshold", DoubleType(), True),
    StructField("context", StringType(), True),
    StructField("created_at", TimestampType(), False),
    StructField("is_acknowledged", BooleanType(), False),
    StructField("acknowledged_by", StringType(), True),
    StructField("acknowledged_at", TimestampType(), True)
])

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Alert Detection Functions

# COMMAND ----------

# DBTITLE 1,Alert Detection Engine
class AlertEngine:
    def __init__(self, config):
        self.config = config
        self.alerts = []
    
    def create_alert(self, alert_type, severity, title, message, 
                     metric_name=None, metric_value=None, threshold=None, context=None):
        alert = {
            "alert_id": str(uuid.uuid4()),
            "alert_type": alert_type,
            "severity": severity,
            "title": title,
            "message": message,
            "metric_name": metric_name,
            "metric_value": float(metric_value) if metric_value is not None else None,
            "threshold": float(threshold) if threshold is not None else None,
            "context": json.dumps(context) if context else None,
            "created_at": datetime.now(),
            "is_acknowledged": False,
            "acknowledged_by": None,
            "acknowledged_at": None
        }
        self.alerts.append(alert)
        return alert
    
    def get_alerts_df(self):
        # Sá»­ dá»¥ng ALERT_SCHEMA Ä‘á»ƒ trÃ¡nh lá»—i inference
        if not self.alerts:
            return spark.createDataFrame([], ALERT_SCHEMA)
        return spark.createDataFrame(self.alerts, ALERT_SCHEMA)
    
    def clear_alerts(self):
        self.alerts = []

alert_engine = AlertEngine(config)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Revenue Change Detection

# COMMAND ----------

# DBTITLE 1,Detect Revenue Changes
def detect_revenue_changes():
    print(f"ğŸ“Š Analyzing revenue changes...")
    try:
        # Sá»­ dá»¥ng fact_orders_streaming join silver.payments
        daily_sales = spark.table(f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.fact_orders_streaming") \
            .join(spark.table(f"{config.CATALOG}.{config.SILVER_SCHEMA}.payments"), "order_id") \
            .withColumn("order_date", to_date("order_purchase_timestamp")) \
            .groupBy("order_date").agg(sum("payment_value").alias("gross_revenue"))
        
        window_spec = Window.orderBy("order_date").rowsBetween(-config.LOOKBACK_DAYS, -1)
        analysis = daily_sales.withColumn("rolling_avg", avg("gross_revenue").over(window_spec)) \
                              .withColumn("pct_change", (col("gross_revenue") - col("rolling_avg")) / col("rolling_avg")) \
                              .filter(col("rolling_avg").isNotNull())
        
        results = analysis.filter((col("pct_change") > config.SPIKE_THRESHOLD) | 
                                  (col("pct_change") < config.DROP_THRESHOLD)).collect()
        
        for row in results:
            alert_type = "revenue_spike" if row['pct_change'] > 0 else "revenue_drop"
            alert_engine.create_alert(
                alert_type=alert_type, severity="critical" if alert_type == "revenue_drop" else "warning",
                title=f"Revenue {alert_type.split('_')[1].capitalize()}: {row['order_date']}",
                message=f"Change of {row['pct_change']*100:.1f}% vs {config.LOOKBACK_DAYS}-day avg",
                metric_name="gross_revenue", metric_value=row['gross_revenue'],
                threshold=config.SPIKE_THRESHOLD if row['pct_change'] > 0 else config.DROP_THRESHOLD
            )
    except Exception as e: print(f"âš ï¸ Revenue Change Error: {e}")

def generate_ml_anomaly_alerts():
    print(f"ğŸ” Scanning ML Anomalies...")
    try:
        anomalies = spark.table(f"{config.CATALOG}.{config.ML_SCHEMA}.revenue_anomalies")
        recent = anomalies.filter("is_anomaly == 1").orderBy(desc("ds")).limit(10).collect()
        for row in recent:
            alert_engine.create_alert(
                alert_type="ml_anomaly", severity="critical",
                title=f"AI Anomaly: {row['ds']}",
                message=f"Isolation Forest detected revenue R$ {row['revenue']:,.2f} as outlier.",
                metric_name="daily_revenue", metric_value=row['revenue'],
                context={"ds": str(row['ds']), "score": row['anomaly_score']}
            )
    except Exception as e: print(f"âš ï¸ ML Anomaly Error: {e}")

def check_data_quality():
    print(f"ğŸ“Š Checking Data Quality...")
    # Sá»­ dá»¥ng Ä‘Ãºng tÃªn cá»™t Ä‘Ã£ phÃ¡t hiá»‡n: _silver_timestamp vÃ  _silver_at
    tables = [
        (f"{config.CATALOG}.{config.SILVER_SCHEMA}.orders", "_silver_timestamp", 24),
        (f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.fact_orders_streaming", "_silver_at", 12)
    ]
    for tab, col_name, hrs in tables:
        try:
            df = spark.table(tab)
            latest = df.agg(max(col_name)).collect()[0][0]
            if latest:
                hrs_old = (datetime.now() - latest).total_seconds() / 3600
                if hrs_old > hrs:
                    alert_engine.create_alert(
                        alert_type="data_quality", severity="warning",
                        title=f"Stale Data: {tab}",
                        message=f"Data is {hrs_old:.1f} hours old (Threshold: {hrs}h)",
                        metric_name="freshness_hours", metric_value=hrs_old, threshold=float(hrs)
                    )
        except Exception as e: print(f"âš ï¸ DQ Error {tab}: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ML Anomaly Alerts

# COMMAND ----------

# DBTITLE 1,Generate ML Anomaly Alerts
def generate_ml_anomaly_alerts():
    """Táº¡o cáº£nh bÃ¡o tá»« káº¿t quáº£ phÃ¡t hiá»‡n báº¥t thÆ°á»ng cá»§a mÃ´ hÃ¬nh ML (Isolation Forest)."""
    
    print(f"ğŸ” Äang quÃ©t káº¿t quáº£ tá»« báº£ng ML: revenue_anomalies...")
    
    try:
        # Äá»c káº¿t quáº£ tá»« Notebook 08
        anomalies = spark.table(f"{config.CATALOG}.{config.ML_SCHEMA}.revenue_anomalies")
        
        recent_anomalies = anomalies.filter("is_anomaly == 1").orderBy(desc("ds")).limit(5).collect()
        
        for row in recent_anomalies:
            alert_engine.create_alert(
                alert_type="ml_anomaly",
                severity="critical",
                title=f"AI phÃ¡t hiá»‡n báº¥t thÆ°á»ng: {row['ds']}",
                message=f"MÃ´ hÃ¬nh Isolation Forest xÃ¡c Ä‘á»‹nh doanh thu R$ {row['revenue']:,.2f} lÃ  ngoáº¡i lai.",
                metric_name="daily_revenue",
                metric_value=row['revenue'],
                context={"ds": str(row['ds']), "anomaly_score": row['anomaly_score']}
            )
        print(f"âœ… ÄÃ£ tÃ­ch há»£p {len(recent_anomalies)} cáº£nh bÃ¡o tá»« ML.")
    except Exception as e:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y báº£ng ML hoáº·c cÃ³ lá»—i: {e}")

generate_ml_anomaly_alerts()

# COMMAND ----------

# MAGIC %md
# MAGIC ### KPI Threshold Alerts

# COMMAND ----------

# DBTITLE 1,Check KPI Thresholds
def check_kpi_thresholds():
    """Kiá»ƒm tra cÃ¡c KPI váº­n hÃ nh so vá»›i ngÆ°á»¡ng cho phÃ©p."""
    
    print(f"ğŸ“Š Äang kiá»ƒm tra ngÆ°á»¡ng KPI...")
    
    # Giáº£ Ä‘á»‹nh báº¡n cÃ³ báº£ng daily_kpis hoáº·c tÃ­nh toÃ¡n trá»±c tiáº¿p tá»« Fact
    try:
        # VÃ­ dá»¥ kiá»ƒm tra tá»· lá»‡ giao hÃ ng muá»™n (Late Delivery Rate)
        fact_orders = spark.table(f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.fact_orders_streaming")
        
        late_rate = fact_orders.select(avg(col("is_late_delivery").cast("float"))).collect()[0][0] * 100
        
        if late_rate > 15: # NgÆ°á»¡ng 15%
            alert_engine.create_alert(
                alert_type="kpi_threshold",
                severity="warning",
                title="Tá»· lá»‡ giao hÃ ng muá»™n cao",
                message=f"Tá»· lá»‡ hiá»‡n táº¡i ({late_rate:.1f}%) vÆ°á»£t ngÆ°á»¡ng 15%",
                metric_name="late_delivery_rate",
                metric_value=late_rate,
                threshold=15.0
            )
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi kiá»ƒm tra KPI: {e}")

check_kpi_thresholds()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Data Quality Alerts

# COMMAND ----------

# DBTITLE 1,Check Data Quality
# Check Data Quality (Corrected Column Names)
def check_data_quality():
    """Kiá»ƒm tra Ä‘á»™ tÆ°Æ¡i (freshness) dá»±a trÃªn Ä‘Ãºng Schema thá»±c táº¿."""
    
    print(f"\n{'='*60}")
    print("ğŸ“Š CHECKING DATA QUALITY")
    print(f"{'='*60}")
    
    alerts = []
    
    # DANH SÃCH ÄÃƒ Cáº¬P NHáº¬T: (tÃªn_báº£ng, cá»™t_thá»±c_táº¿, ngÆ°á»¡ng_giá»)
    tables_to_check = [
        (f"{config.CATALOG}.{config.SILVER_SCHEMA}.orders", "_silver_timestamp", 24),
        (f"{config.CATALOG}.{config.SILVER_SCHEMA}.order_items", "_silver_timestamp", 24),
        (f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.fact_orders_streaming", "_silver_at", 12)
    ]
    
    for table_full_name, timestamp_col, max_hours in tables_to_check:
        try:
            df = spark.table(table_full_name)
            
            # 1. Kiá»ƒm tra Ä‘á»™ tÆ°Æ¡i (Freshness)
            latest_ts = df.agg(max(timestamp_col)).collect()[0][0]
            
            if latest_ts:
                hours_old = (datetime.now() - latest_ts).total_seconds() / 3600
                
                if hours_old > max_hours:
                    alert = alert_engine.create_alert(
                        alert_type="data_quality_stale",
                        severity="warning",
                        title=f"Dá»¯ liá»‡u bá»‹ cháº­m: {table_full_name}",
                        message=f"Báº£ng chÆ°a Ä‘Æ°á»£c cáº­p nháº­t trong {hours_old:.1f} giá» (NgÆ°á»¡ng: {max_hours}h)",
                        metric_name="data_freshness_hours",
                        metric_value=hours_old,
                        threshold=float(max_hours),
                        context={"table": table_full_name, "last_update": str(latest_ts)}
                    )
                    alerts.append(alert)
                    print(f"  âš ï¸ {table_full_name}: {hours_old:.1f} giá» tuá»•i (Cáº£nh bÃ¡o)")
                else:
                    print(f"  âœ… {table_full_name}: {hours_old:.1f} giá» tuá»•i (á»”n Ä‘á»‹nh)")
            
            # 2. Kiá»ƒm tra tÃ­nh Ä‘áº§y Ä‘á»§ (Row count)
            if df.count() == 0:
                alert = alert_engine.create_alert(
                    alert_type="data_quality_empty",
                    severity="critical",
                    title=f"Báº£ng trá»‘ng: {table_full_name}",
                    message=f"PhÃ¡t hiá»‡n báº£ng khÃ´ng cÃ³ dá»¯ liá»‡u",
                    metric_name="row_count",
                    metric_value=0,
                    threshold=1
                )
                alerts.append(alert)
                print(f"  ğŸš¨ {table_full_name}: Báº¢NG TRá»NG!")
                
        except Exception as e:
            # In lá»—i chi tiáº¿t Ä‘á»ƒ debug náº¿u váº«n gáº·p váº¥n Ä‘á» column
            print(f"  âŒ Lá»—i kiá»ƒm tra {table_full_name}: {str(e)[:100]}...")
    
    return alerts

# Cháº¡y láº¡i hÃ m kiá»ƒm tra
dq_alerts = check_data_quality()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“¤ Notification Functions

# COMMAND ----------

# DBTITLE 1,Notification Functions
def send_notifications(alerts):
    if not alerts: 
        print("âœ… No alerts to send.")
        return
    for a in alerts:
        icon = "ğŸš¨" if a["severity"] == "critical" else "âš ï¸"
        print(f"{icon} {a['title']}: {a['message']}")

def save_alerts():
    df = alert_engine.get_alerts_df()
    if df.count() > 0:
        target = f"{config.CATALOG}.{config.BUSINESS_SCHEMA}.alert_history"
        df.withColumn("_updated_at", current_timestamp()) \
          .write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target)
        print(f"âœ… Saved {df.count()} alerts to {target}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ Save Alerts

# COMMAND ----------

# DBTITLE 1,Save Alerts to Delta Table
alert_engine.clear_alerts()
detect_revenue_changes()
generate_ml_anomaly_alerts()
check_data_quality()
save_alerts()
send_notifications(alert_engine.alerts)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Alert Summary Dashboard

# COMMAND ----------

# DBTITLE 1,Display Alert Summary
def display_alert_summary():
    """Display summary of all generated alerts."""
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“Š ALERT SUMMARY                                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£""")
    
    alerts_df = alert_engine.get_alerts_df()
    
    if alerts_df is None or alerts_df.count() == 0:
        print("â•‘  âœ… No alerts generated - All systems normal                          â•‘")
    else:
        # Count by severity
        severity_counts = alerts_df.groupBy("severity").count().collect()
        severity_dict = {row["severity"]: row["count"] for row in severity_counts}
        
        critical = severity_dict.get("critical", 0)
        warning = severity_dict.get("warning", 0)
        info = severity_dict.get("info", 0)
        
        print(f"â•‘  ğŸš¨ Critical:  {critical:<58} â•‘")
        print(f"â•‘  âš ï¸  Warning:   {warning:<58} â•‘")
        print(f"â•‘  â„¹ï¸  Info:      {info:<58} â•‘")
        print(f"â•‘  ğŸ“Š Total:     {alerts_df.count():<58} â•‘")
        
        print(f"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        
        # Count by type
        type_counts = alerts_df.groupBy("alert_type").count().orderBy(desc("count")).collect()
        
        print("â•‘  Alert Types:                                                             â•‘")
        for row in type_counts:
            print(f"â•‘    â€¢ {row['alert_type']:<30} {row['count']:>5} alerts{' '*23} â•‘")
    
    print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

display_alert_summary()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ View Recent Alerts

# COMMAND ----------

# DBTITLE 1,View Recent Alerts
alerts_df = alert_engine.get_alerts_df()

if alerts_df and alerts_df.count() > 0:
    print("\nğŸ“‹ RECENT ALERTS:")
    display(
        alerts_df.select(
            "created_at",
            "severity",
            "alert_type",
            "title",
            "metric_value",
            "threshold"
        ).orderBy(desc("created_at"))
    )
else:
    print("\nâœ… No alerts generated - All systems are operating normally!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ Run Complete Alert Check

# COMMAND ----------

# DBTITLE 1,Run Complete Alert Check
# DBTITLE 1,ğŸš€ Final Summary Verification
# 1. Láº¥y DataFrame cáº£nh bÃ¡o má»™t cÃ¡ch an toÃ n
alerts_final_df = alert_engine.get_alerts_df()

# 2. TÃ­nh toÃ¡n tá»•ng sá»‘ cáº£nh bÃ¡o (Xá»­ lÃ½ trÆ°á»ng há»£p DataFrame rá»—ng hoáº·c None)
if alerts_final_df is not None:
    total_alerts = alerts_final_df.count()
else:
    total_alerts = 0

# 3. In báº£ng tÃ³m táº¯t chuyÃªn nghiá»‡p
print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸš¨ REVENUE ALERTS COMPLETE                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  âœ… Status:        SUCCESS                                                    â•‘
â•‘  ğŸ“Š Alerts:        {total_alerts} alerts generated{' ' * (45 - len(str(total_alerts)))}      â•‘
â•‘                                                                               â•‘
â•‘  ğŸ“‹ Alert Types Implemented:                                                  â•‘
â•‘     1. Revenue Spike Detection (>{config.SPIKE_THRESHOLD*100:.0f}% increase){' ' * 31} â•‘
â•‘     3. ML Anomaly Alerts (Isolation Forest)                                   â•‘
â•‘     4. KPI Threshold Alerts (Delivery, Cancellation, Late Delivery)           â•‘
â•‘     5. Data Quality Alerts (Freshness & Completeness)                         â•‘
â•‘                                                                               â•‘
â•‘  ğŸ“¤ Notification Channels:                                                    â•‘
â•‘     â€¢ Console output (Active)                                                 â•‘
â•‘     â€¢ Slack webhook (Configured: {"Yes" if config.SLACK_WEBHOOK else "No"})   â•‘
â•‘     â€¢ Email (Enabled: {config.EMAIL_ENABLED})                                 â•‘
â•‘                                                                               â•‘
â•‘  ğŸ’¾ Alert History (Unity Catalog):                                            â•‘
â•‘     â€¢ Table: {config.CATALOG}.{config.BUSINESS_SCHEMA}.alert_history          â•‘
â•‘                                                                               â•‘
â•‘  â­ï¸  Next Step:     Run 10_data_quality.py                                    â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Thay vÃ¬ dÃ¹ng assert gÃ¢y crash notebook, hÃ£y dÃ¹ng thÃ´ng bÃ¡o info
if total_alerts == 0:
    print("â„¹ï¸ All systems are operating normally. No anomalies detected.")